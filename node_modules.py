
import torch.nn as nn
import torch.nn.functional as F
import torch




class Dense_GIN(nn.Module):

    def __init__(self,num_layer,input_dim,output_dim,hidden_dim=0,eps: float = 0.0,train_eps: bool = False,):
        super(Dense_GIN, self).__init__()

        self.num_layer = num_layer
        if hidden_dim == 0:
            hidden_dim = output_dim

        self.model = nn.ModuleList()

        if self.num_layer == 1:
            emdedimg_mlp = nn.Sequential(
                nn.Linear(in_features=input_dim, out_features=output_dim),
                nn.ReLU6()
            )
            self.model.append(Dense_GIN_layer(emdedimg_mlp))
        elif self.num_layer == 2:
            emdedimg_mlp = nn.Sequential(
                nn.Linear(in_features=input_dim, out_features=hidden_dim,bias=True),
                nn.ReLU6()
            )
            out_mlp = nn.Sequential(
                nn.Linear(in_features=hidden_dim, out_features=output_dim,bias=True),
                nn.ReLU6()
            )
            self.model.append(Dense_GIN_layer(emdedimg_mlp))
            self.model.append(Dense_GIN_layer(out_mlp))
        else:
            emdedimg_mlp = nn.Sequential(
                nn.Linear(in_features=input_dim, out_features=hidden_dim,bias=True),
                nn.ReLU6()
            )
            hidden_mlp = nn.Sequential(
                nn.Linear(in_features=hidden_dim, out_features=hidden_dim,bias=True),
                nn.ReLU6()
            )
            out_mlp = nn.Sequential(
                nn.Linear(in_features=hidden_dim, out_features=output_dim,bias=True),
                nn.ReLU6()
            )
            self.model.append(Dense_GIN_layer(emdedimg_mlp))
            for i in range(1, self.num_layer-1):
                self.model.append(Dense_GIN_layer(hidden_mlp))
            self.model.append(Dense_GIN_layer(out_mlp))


    def forward(self, x, adj):
        out = x
        for i in range(self.num_layer):
            out = self.model[i](out, adj)

        return out


class Dense_GIN_layer(nn.Module):
    r"""See :class:`torch_geometric.nn.conv.GINConv`."""
    def __init__(self, nn, eps: float = 0.0, train_eps: bool = False,
    ):
        super(Dense_GIN_layer,self).__init__()

        self.nn = nn
        self.initial_eps = eps
        if train_eps:
            self.eps = torch.nn.Parameter(torch.Tensor([eps]))
        else:
            self.register_buffer('eps', torch.Tensor([eps]))
        self.eps.data.fill_(self.initial_eps)



    def forward(self, x, adj,
                add_loop: bool = True):

        N, _ = adj.size()

        out = torch.matmul(adj, x)
        if add_loop:
            out = (1 + self.eps) * x + out

        out = self.nn(out)

        return out


class attention_layer(nn.Module):

    def __init__(self, dim, num_heads, lpapas_ratio, qkv_bias):
        super(attention_layer, self).__init__()

        self.dim = dim
        self.num_heads = num_heads
        self.qkv_bias = qkv_bias
        if lpapas_ratio == -1:
            self.lpapas_ratio = nn.Parameter(torch.Tensor([0.5]))
        else:
            self.lpapas_ratio = lpapas_ratio

        head_dim = self.dim // self.num_heads

        self.scale = head_dim ** -0.5
        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=self.qkv_bias)

        self.attn_drop = nn.Dropout(0.3)

    def forward(self, x, adj):


        N, C = x.shape
        qkv = self.qkv(x).reshape(N, 3, self.num_heads, C // self.num_heads).permute(1, 2, 0, 3)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale

        deg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)

        lpapas = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)

        attn = attn.softmax(dim=-1) * (1 - self.lpapas_ratio) + self.lpapas_ratio * lpapas.view(1,N,N).softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(N, C)

        return x


class Block(nn.Module):

    def __init__(self,dim,num_heads,lpapas_ratio,qkv_bias):
        super(Block,self).__init__()

        self.attention_layer = attention_layer(dim,num_heads,lpapas_ratio,qkv_bias)

        self.forward_liner = nn.Sequential(
            nn.Linear(dim,dim),
            nn.ReLU6(),
        )

    def forward(self,x,adj):

        x = F.layer_norm(x,[x.shape[-1]])

        out_1 = F.layer_norm(x + F.layer_norm(self.attention_layer(x,adj),[x.shape[-1]]),[x.shape[-1]])

        out = F.layer_norm(out_1 + self.forward_liner(out_1), [x.shape[-1]])

        return out


class graph_transformer(nn.Module):

    def __init__(self, conf):
        super(graph_transformer,self).__init__()

        self.num_layer = conf.num_layers

        self.block_list = nn.ModuleList()
        for i in range(self.num_layer):

            self.block_list.append(Block(conf.hidden_dim, conf.num_heads, conf.lpapas_ratio, conf.qkv_bias))


    def forward(self, x, adj):

        for i in range(self.num_layer):
            x = self.block_list[i](x, adj)
        return x

